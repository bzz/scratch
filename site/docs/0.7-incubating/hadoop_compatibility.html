<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Apache Flink (incubating): Hadoop Compatibility</title>
    <link rel="shortcut icon" href="favicon.ico" type="image/x-icon">
    <link rel="icon" href="favicon.ico" type="image/x-icon">
    <link rel="stylesheet" href="css/bootstrap.css">
    <link rel="stylesheet" href="css/bootstrap-lumen-custom.css">
    <link rel="stylesheet" href="css/syntax.css">
    <link rel="stylesheet" href="css/custom.css">
    <link href="css/main/main.css" rel="stylesheet">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <script src="js/codetabs.js"></script>
  </head>
  <body>

    <nav class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="row">
      <div class="col-md-1 af-mobile-nav-bar">
	<a href="index.html" title="Home">
	  <img class="hidden-xs hidden-sm img-responsive"
	       src="img/logo.png" alt="Apache Flink Logo">
	</a>	
	<div class="row visible-xs">
	  <div class="col-xs-3">
	    <a href="index.html" title="Home">  
	      <img class="hidden-x hidden-sm img-responsive"
		   src="img/logo.png" alt="Apache Flink Logo">
	    </a>	
	  </div>
	  <div class="col-xs-5"></div>
	  <div class="col-xs-4">
	    <div class="af-mobile-btn">
	      <span class="glyphicon glyphicon-plus"></span>
	    </div>
	  </div>
	</div>
      </div>
      <!-- Navigation -->
      <div class="col-md-11">
	<div class="collapse navbar-collapse" id="navbar-collapse-1">
	  <ul class="nav navbar-nav">

	    <li>
	      <a href="index.html" class="">Documentation</a>
	    </li>

	    <li>
	      <a href="api/java/index.html">Javadoc</a>
	    </li>

	    <li>
	      <a href="api/scala/index.html#org.apache.flink.api.scala.package">Scaladoc</a>
	    </li>

	  </ul>
	</div>
      </div>
    </div>
  </div>
</nav>


    <div style="padding-top:120px" class="container">

      <div class="row">
        <div class="col-md-3">
          <ul>
  <li><a href="faq.html">FAQ</a></li>
  <li>Quickstart
    <ul>
      <li><a href="setup_quickstart.html">Setup</a></li>
      <li><a href="run_example_quickstart.html">Run Example</a></li>
      <li><a href="java_api_quickstart.html">Java API</a></li>
      <li><a href="scala_api_quickstart.html">Scala API</a></li>
    </ul>
  </li>

  <li>Setup &amp; Configuration
    <ul>
      <li><a href="local_setup.html">Local Setup</a></li>
      <li><a href="building.html">Build Flink</a></li>
      <li><a href="cluster_setup.html">Cluster Setup</a></li>
      <li><a href="yarn_setup.html">YARN Setup</a></li>
      <li><a href="config.html">Configuration</a></li>
    </ul>
  </li>

  <li>Programming Guides
    <ul>
      <li><a href="programming_guide.html">Programming Guide</a></li>
      <li><a href="dataset_transformations.html">DataSet Transformations</a></li>
      <li><a href="java8_programming_guide.html">Java 8 Programming Guide</a></li>
      <li><a href="streaming_guide.html">Streaming Guide</a></li>
      <li><a href="iterations.html">Iterations</a></li>
      <li><a href="spargel_guide.html">Spargel Graph API</a></li>
      <li><a href="hadoop_compatibility.html">Hadoop Compatibility</a></li>
    </ul>
  </li>

  <li>Examples
    <ul>
      <li><a href="examples.html">Bundled Examples</a></li>
      <li><a href="example_connectors.html">Connecting to other systems</a></li>
    </ul>
  </li>

  <li>Execution
    <ul>
      <li><a href="local_execution.html">Local/Debugging</a></li>
      <li><a href="cluster_execution.html">Cluster</a></li>
      <li><a href="cli.html">Command-Line Interface</a></li>
      <li><a href="web_client.html">Web Interface</a></li>
    </ul>
  </li>

  <li>Internals
    <ul>
      <li><a href="internal_overview.html">Overview</a></li>
    </ul>
  </li>
</ul>

        </div>  
        <div class="col-md-9">
          <h1>Hadoop Compatibility</h1>
	  
          <ul id="markdown-toc">
  <li><a href="#project-configuration">Project Configuration</a></li>
  <li><a href="#using-hadoop-data-types">Using Hadoop Data Types</a></li>
  <li><a href="#using-hadoop-inputformats">Using Hadoop InputFormats</a></li>
  <li><a href="#using-hadoop-outputformats">Using Hadoop OutputFormats</a></li>
  <li><a href="#using-hadoop-mappers-and-reducers">Using Hadoop Mappers and Reducers</a></li>
  <li><a href="#complete-hadoop-wordcount-example">Complete Hadoop WordCount Example</a></li>
</ul>

<p>Flink is compatible with many Apache Hadoop’s MapReduce interfaces and allows to reuse a lot of code that was implemented for Hadoop MapReduce.</p>

<p>You can:</p>

<ul>
  <li>use Hadoop’s <code>Writable</code> <a href="programming_guide.html#data-types">data types</a> in Flink programs.</li>
  <li>use any Hadoop <code>InputFormat</code> as a <a href="programming_guide.html#data-sources">DataSource</a>.</li>
  <li>use any Hadoop <code>OutputFormat</code> as a <a href="programming_guide.html#data-sinks">DataSink</a>.</li>
  <li>use a Hadoop <code>Mapper</code> as <a href="dataset_transformations.html#flatmap">FlatMapFunction</a>.</li>
  <li>use a Hadoop <code>Reducer</code> as <a href="dataset_transformations.html#groupreduce-on-grouped-dataset">GroupReduceFunction</a>.</li>
</ul>

<p>This document shows how to use existing Hadoop MapReduce code with Flink.</p>

<h3 id="project-configuration">Project Configuration</h3>

<p>The Hadoop Compatibility Layer is part of the <code>flink-addons</code> Maven module. All relevant classes are located in the <code>org.apache.flink.hadoopcompatibility</code> package. It includes separate packages and classes for the Hadoop <code>mapred</code> and <code>mapreduce</code> APIs.</p>

<p>Add the following dependency to your <code>pom.xml</code> to use the Hadoop Compatibility Layer.</p>

<div class="highlight"><pre><code class="language-xml"><span class="nt">&lt;dependency&gt;</span>
	<span class="nt">&lt;groupId&gt;</span>org.apache.flink<span class="nt">&lt;/groupId&gt;</span>
	<span class="nt">&lt;artifactId&gt;</span>flink-hadoop-compatibility<span class="nt">&lt;/artifactId&gt;</span>
	<span class="nt">&lt;version&gt;</span>0.7.0-incubating<span class="nt">&lt;/version&gt;</span>
<span class="nt">&lt;/dependency&gt;</span></code></pre></div>

<h3 id="using-hadoop-data-types">Using Hadoop Data Types</h3>

<p>Flink supports all Hadoop <code>Writable</code> and <code>WritableComparable</code> data types out-of-the-box. You do not need to include the Hadoop Compatibility dependency, if you only want to use your Hadoop data types. See the <a href="programming_guide.html#data-types">Programming Guide</a> for more details.</p>

<h3 id="using-hadoop-inputformats">Using Hadoop InputFormats</h3>

<p>Flink provides a compatibility wrapper for Hadoop <code>InputFormats</code>. Any class that implements <code>org.apache.hadoop.mapred.InputFormat</code> or extends <code>org.apache.hadoop.mapreduce.InputFormat</code> is supported. Thus, Flink can handle Hadoop built-in formats such as <code>TextInputFormat</code> as well as external formats such as Hive’s <code>HCatInputFormat</code>. Data read from Hadoop InputFormats is converted into a <code>DataSet&lt;Tuple2&lt;KEY,VALUE&gt;&gt;</code> where <code>KEY</code> is the key and <code>VALUE</code> is the value of the original Hadoop key-value pair.</p>

<p>Flink’s InputFormat wrappers are </p>

<ul>
  <li><code>org.apache.flink.hadoopcompatibility.mapred.HadoopInputFormat</code> and </li>
  <li><code>org.apache.flink.hadoopcompatibility.mapreduce.HadoopInputFormat</code></li>
</ul>

<p>and can be used as regular Flink <a href="programming_guide.html#data-sources">InputFormats</a>.</p>

<p>The following example shows how to use Hadoop’s <code>TextInputFormat</code>.</p>

<div class="highlight"><pre><code class="language-java"><span class="n">ExecutionEnvironment</span> <span class="n">env</span> <span class="o">=</span> <span class="n">ExecutionEnvironment</span><span class="o">.</span><span class="na">getExecutionEnvironment</span><span class="o">();</span>
		
<span class="c1">// Set up the Hadoop TextInputFormat.</span>
<span class="n">Job</span> <span class="n">job</span> <span class="o">=</span> <span class="n">Job</span><span class="o">.</span><span class="na">getInstance</span><span class="o">();</span>
<span class="n">HadoopInputFormat</span><span class="o">&lt;</span><span class="n">LongWritable</span><span class="o">,</span> <span class="n">Text</span><span class="o">&gt;</span> <span class="n">hadoopIF</span> <span class="o">=</span> 
  <span class="c1">// create the Flink wrapper.</span>
  <span class="k">new</span> <span class="n">HadoopInputFormat</span><span class="o">&lt;</span><span class="n">LongWritable</span><span class="o">,</span> <span class="n">Text</span><span class="o">&gt;(</span>
    <span class="c1">// create the Hadoop InputFormat, specify key and value type, and job.</span>
    <span class="k">new</span> <span class="nf">TextInputFormat</span><span class="o">(),</span> <span class="n">LongWritable</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="n">Text</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="n">job</span>
  <span class="o">);</span>
<span class="n">TextInputFormat</span><span class="o">.</span><span class="na">addInputPath</span><span class="o">(</span><span class="n">job</span><span class="o">,</span> <span class="k">new</span> <span class="nf">Path</span><span class="o">(</span><span class="n">inputPath</span><span class="o">));</span>
		
<span class="c1">// Read data using the Hadoop TextInputFormat.</span>
<span class="n">DataSet</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">LongWritable</span><span class="o">,</span> <span class="n">Text</span><span class="o">&gt;&gt;</span> <span class="n">text</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="na">createInput</span><span class="o">(</span><span class="n">hadoopIF</span><span class="o">);</span>

<span class="c1">// Do something with the data.</span>
<span class="o">[...]</span></code></pre></div>

<h3 id="using-hadoop-outputformats">Using Hadoop OutputFormats</h3>

<p>Flink provides a compatibility wrapper for Hadoop <code>OutputFormats</code>. Any class that implements <code>org.apache.hadoop.mapred.OutputFormat</code> or extends <code>org.apache.hadoop.mapreduce.OutputFormat</code> is supported. The OutputFormat wrapper expects its input data to be a <code>DataSet&lt;Tuple2&lt;KEY,VALUE&gt;&gt;</code> where <code>KEY</code> is the key and <code>VALUE</code> is the value of the Hadoop key-value pair that is processed by the Hadoop OutputFormat.</p>

<p>Flink’s OUtputFormat wrappers are</p>

<ul>
  <li><code>org.apache.flink.hadoopcompatibility.mapred.HadoopOutputFormat</code> and </li>
  <li><code>org.apache.flink.hadoopcompatibility.mapreduce.HadoopOutputFormat</code></li>
</ul>

<p>and can be used as regular Flink <a href="programming_guide.html#data-sinks">OutputFormats</a>.</p>

<p>The following example shows how to use Hadoop’s <code>TextOutputFormat</code>.</p>

<div class="highlight"><pre><code class="language-java"><span class="c1">// Obtain your result to emit.</span>
<span class="n">DataSet</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Text</span><span class="o">,</span> <span class="n">IntWritable</span><span class="o">&gt;&gt;</span> <span class="n">hadoopResult</span> <span class="o">=</span> <span class="o">[...]</span>
		
<span class="c1">// Set up the Hadoop TextOutputFormat.</span>
<span class="n">HadoopOutputFormat</span><span class="o">&lt;</span><span class="n">Text</span><span class="o">,</span> <span class="n">IntWritable</span><span class="o">&gt;</span> <span class="n">hadoopOF</span> <span class="o">=</span> 
  <span class="c1">// create the Flink wrapper.</span>
  <span class="k">new</span> <span class="n">HadoopOutputFormat</span><span class="o">&lt;</span><span class="n">Text</span><span class="o">,</span> <span class="n">IntWritable</span><span class="o">&gt;(</span>
    <span class="c1">// set the Hadoop OutputFormat and specify the job.</span>
    <span class="k">new</span> <span class="n">TextOutputFormat</span><span class="o">&lt;</span><span class="n">Text</span><span class="o">,</span> <span class="n">IntWritable</span><span class="o">&gt;(),</span> <span class="n">job</span>
  <span class="o">);</span>
<span class="n">hadoopOF</span><span class="o">.</span><span class="na">getConfiguration</span><span class="o">().</span><span class="na">set</span><span class="o">(</span><span class="s">&quot;mapreduce.output.textoutputformat.separator&quot;</span><span class="o">,</span> <span class="s">&quot; &quot;</span><span class="o">);</span>
<span class="n">TextOutputFormat</span><span class="o">.</span><span class="na">setOutputPath</span><span class="o">(</span><span class="n">job</span><span class="o">,</span> <span class="k">new</span> <span class="nf">Path</span><span class="o">(</span><span class="n">outputPath</span><span class="o">));</span>
		
<span class="c1">// Emit data using the Hadoop TextOutputFormat.</span>
<span class="n">result</span><span class="o">.</span><span class="na">output</span><span class="o">(</span><span class="n">hadoopOF</span><span class="o">)</span>
      <span class="o">.</span><span class="na">setParallelism</span><span class="o">(</span><span class="mi">1</span><span class="o">);</span></code></pre></div>

<p><strong>Please note:</strong> At the moment, Hadoop OutputFormats must be executed with a parallelism of 1 (DOP = 1). This limitation will be resolved soon.</p>

<h3 id="using-hadoop-mappers-and-reducers">Using Hadoop Mappers and Reducers</h3>

<p>Hadoop Mappers are semantically equivalent to Flink’s <a href="dataset_transformations.html#flatmap">FlatMapFunctions</a> and Hadoop Reducers are equivalent to Flink’s <a href="dataset_transformations.html#groupreduce-on-grouped-dataset">GroupReduceFunctions</a>. Flink provides wrappers for implementations of Hadoop MapReduce’s <code>Mapper</code> and <code>Reducer</code> interfaces, i.e., you can reuse your Hadoop Mappers and Reducers in regular Flink programs. At the moment, only the Mapper and Reduce interfaces of Hadoop’s mapred API (<code>org.apache.hadoop.mapred</code>) are supported.</p>

<p>The wrappers take a <code>DataSet&lt;Tuple2&lt;KEYIN,VALUEIN&gt;&gt;</code> as input and produce a <code>DataSet&lt;Tuple2&lt;KEYOUT,VALUEOUT&gt;&gt;</code> as output where <code>KEYIN</code> and <code>KEYOUT</code> are the keys and <code>VALUEIN</code> and <code>VALUEOUT</code> are the values of the Hadoop key-value pairs that are processed by the Hadoop functions. For Reducers, Flink offers a wrapper for a GroupReduceFunction with (<code>HadoopReduceCombineFunction</code>) and without a Combiner (<code>HadoopReduceFunction</code>). The wrappers accept an optional <code>JobConf</code> object to configure the Hadoop Mapper or Reducer.</p>

<p>Flink’s function wrappers are </p>

<ul>
  <li><code>org.apache.flink.hadoopcompatibility.mapred.HadoopMapFunction</code>,</li>
  <li><code>org.apache.flink.hadoopcompatibility.mapred.HadoopReduceFunction</code>, and</li>
  <li><code>org.apache.flink.hadoopcompatibility.mapred.HadoopReduceCombineFunction</code>.</li>
</ul>

<p>and can be used as regular Flink <a href="dataset_transformations.html#flatmap">FlatMapFunctions</a> or <a href="dataset_transformations.html#groupreduce-on-grouped-dataset">GroupReduceFunctions</a>.</p>

<p>The following example shows how to use Hadoop <code>Mapper</code> and <code>Reducer</code> functions.</p>

<div class="highlight"><pre><code class="language-java"><span class="c1">// Obtain data to process somehow.</span>
<span class="n">DataSet</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Text</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">&gt;&gt;</span> <span class="n">text</span> <span class="o">=</span> <span class="o">[...]</span>

<span class="n">DataSet</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Text</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">&gt;&gt;</span> <span class="n">result</span> <span class="o">=</span> <span class="n">text</span>
  <span class="c1">// use Hadoop Mapper (Tokenizer) as MapFunction</span>
  <span class="o">.</span><span class="na">flatMap</span><span class="o">(</span><span class="k">new</span> <span class="n">HadoopMapFunction</span><span class="o">&lt;</span><span class="n">LongWritable</span><span class="o">,</span> <span class="n">Text</span><span class="o">,</span> <span class="n">Text</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">&gt;(</span>
    <span class="k">new</span> <span class="nf">Tokenizer</span><span class="o">()</span>
  <span class="o">))</span>
  <span class="o">.</span><span class="na">groupBy</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>
  <span class="c1">// use Hadoop Reducer (Counter) as Reduce- and CombineFunction</span>
  <span class="o">.</span><span class="na">reduceGroup</span><span class="o">(</span><span class="k">new</span> <span class="n">HadoopReduceCombineFunction</span><span class="o">&lt;</span><span class="n">Text</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">,</span> <span class="n">Text</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">&gt;(</span>
    <span class="k">new</span> <span class="nf">Counter</span><span class="o">(),</span> <span class="k">new</span> <span class="nf">Counter</span><span class="o">()</span>
  <span class="o">));</span></code></pre></div>

<p><strong>Please note:</strong> The Reducer wrapper works on groups as defined by Flink’s <a href="dataset_transformations.html#transformations-on-grouped-dataset">groupBy()</a> operation. It does not consider any custom partitioners, sort or grouping comparators you might have set in the <code>JobConf</code>. </p>

<h3 id="complete-hadoop-wordcount-example">Complete Hadoop WordCount Example</h3>

<p>The following example shows a complete WordCount implementation using Hadoop data types, Input- and OutputFormats, and Mapper and Reducer implementations.</p>

<div class="highlight"><pre><code class="language-java"><span class="n">ExecutionEnvironment</span> <span class="n">env</span> <span class="o">=</span> <span class="n">ExecutionEnvironment</span><span class="o">.</span><span class="na">getExecutionEnvironment</span><span class="o">();</span>
		
<span class="c1">// Set up the Hadoop TextInputFormat.</span>
<span class="n">Job</span> <span class="n">job</span> <span class="o">=</span> <span class="n">Job</span><span class="o">.</span><span class="na">getInstance</span><span class="o">();</span>
<span class="n">HadoopInputFormat</span><span class="o">&lt;</span><span class="n">LongWritable</span><span class="o">,</span> <span class="n">Text</span><span class="o">&gt;</span> <span class="n">hadoopIF</span> <span class="o">=</span> 
  <span class="k">new</span> <span class="n">HadoopInputFormat</span><span class="o">&lt;</span><span class="n">LongWritable</span><span class="o">,</span> <span class="n">Text</span><span class="o">&gt;(</span>
    <span class="k">new</span> <span class="nf">TextInputFormat</span><span class="o">(),</span> <span class="n">LongWritable</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="n">Text</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="n">job</span>
  <span class="o">);</span>
<span class="n">TextInputFormat</span><span class="o">.</span><span class="na">addInputPath</span><span class="o">(</span><span class="n">job</span><span class="o">,</span> <span class="k">new</span> <span class="nf">Path</span><span class="o">(</span><span class="n">inputPath</span><span class="o">));</span>
		
<span class="c1">// Read data using the Hadoop TextInputFormat.</span>
<span class="n">DataSet</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">LongWritable</span><span class="o">,</span> <span class="n">Text</span><span class="o">&gt;&gt;</span> <span class="n">text</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="na">createInput</span><span class="o">(</span><span class="n">hadoopIF</span><span class="o">);</span>

<span class="n">DataSet</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Text</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">&gt;&gt;</span> <span class="n">result</span> <span class="o">=</span> <span class="n">text</span>
  <span class="c1">// use Hadoop Mapper (Tokenizer) as MapFunction</span>
  <span class="o">.</span><span class="na">flatMap</span><span class="o">(</span><span class="k">new</span> <span class="n">HadoopMapFunction</span><span class="o">&lt;</span><span class="n">LongWritable</span><span class="o">,</span> <span class="n">Text</span><span class="o">,</span> <span class="n">Text</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">&gt;(</span>
    <span class="k">new</span> <span class="nf">Tokenizer</span><span class="o">()</span>
  <span class="o">))</span>
  <span class="o">.</span><span class="na">groupBy</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>
  <span class="c1">// use Hadoop Reducer (Counter) as Reduce- and CombineFunction</span>
  <span class="o">.</span><span class="na">reduceGroup</span><span class="o">(</span><span class="k">new</span> <span class="n">HadoopReduceCombineFunction</span><span class="o">&lt;</span><span class="n">Text</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">,</span> <span class="n">Text</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">&gt;(</span>
    <span class="k">new</span> <span class="nf">Counter</span><span class="o">(),</span> <span class="k">new</span> <span class="nf">Counter</span><span class="o">()</span>
  <span class="o">));</span>

<span class="c1">// Set up the Hadoop TextOutputFormat.</span>
<span class="n">HadoopOutputFormat</span><span class="o">&lt;</span><span class="n">Text</span><span class="o">,</span> <span class="n">IntWritable</span><span class="o">&gt;</span> <span class="n">hadoopOF</span> <span class="o">=</span> 
  <span class="k">new</span> <span class="n">HadoopOutputFormat</span><span class="o">&lt;</span><span class="n">Text</span><span class="o">,</span> <span class="n">IntWritable</span><span class="o">&gt;(</span>
    <span class="k">new</span> <span class="n">TextOutputFormat</span><span class="o">&lt;</span><span class="n">Text</span><span class="o">,</span> <span class="n">IntWritable</span><span class="o">&gt;(),</span> <span class="n">job</span>
  <span class="o">);</span>
<span class="n">hadoopOF</span><span class="o">.</span><span class="na">getConfiguration</span><span class="o">().</span><span class="na">set</span><span class="o">(</span><span class="s">&quot;mapreduce.output.textoutputformat.separator&quot;</span><span class="o">,</span> <span class="s">&quot; &quot;</span><span class="o">);</span>
<span class="n">TextOutputFormat</span><span class="o">.</span><span class="na">setOutputPath</span><span class="o">(</span><span class="n">job</span><span class="o">,</span> <span class="k">new</span> <span class="nf">Path</span><span class="o">(</span><span class="n">outputPath</span><span class="o">));</span>
		
<span class="c1">// Emit data using the Hadoop TextOutputFormat.</span>
<span class="n">result</span><span class="o">.</span><span class="na">output</span><span class="o">(</span><span class="n">hadoopOF</span><span class="o">)</span>
      <span class="o">.</span><span class="na">setParallelism</span><span class="o">(</span><span class="mi">1</span><span class="o">);</span>

<span class="c1">// Execute Program</span>
<span class="n">env</span><span class="o">.</span><span class="na">execute</span><span class="o">(</span><span class="s">&quot;Hadoop WordCount&quot;</span><span class="o">);</span></code></pre></div>

	  
        <!-- Disqus Area -->
          <div style="padding-top:30px" id="disqus_thread"></div>
      
            <script type="text/javascript">
                /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
                var disqus_shortname = 'stratosphere-eu'; // required: replace example with your forum shortname

                /* * * DON'T EDIT BELOW THIS LINE * * */
                (function() {
                    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
                    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
                    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
                })();
            </script>
            <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
          </div>
        </div>

        <div class="footer">
          
          <hr class="divider" />

<p><small>Apache Flink is an effort undergoing incubation at The Apache Software
Foundation (ASF), sponsored by the Apache Incubator PMC. Incubation is
required of all newly accepted projects until a further review indicates that
the infrastructure, communications, and decision making process have
stabilized in a manner consistent with other successful ASF projects. While
incubation status is not necessarily a reflection of the completeness or
stability of the code, it does indicate that the project has yet to be fully
endorsed by the ASF.</small></p>

<p><a href="http://incubator.apache.org/"><img src="/img/apache-incubator-logo.png" alt="Incubator Logo" /></a></p>

<p class="text-center"><a href="privacy-policy.html">Privacy Policy<a>
</a></a></p>

        </div>
      </div>
    </div>

    

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-52545728-1', 'auto');
      ga('send', 'pageview');

    </script>

  </body>
</html>
